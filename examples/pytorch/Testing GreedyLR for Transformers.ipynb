{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109ba453-d065-4aa3-ab70-a054b49a28d0",
   "metadata": {},
   "source": [
    "# Testing â­ GreedLR Scheduler for ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4360d05-6e8a-4951-a611-7b4b345a71b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ff579-2386-4114-b011-92c064f6f2ee",
   "metadata": {},
   "source": [
    "### Need to reinstall from source to register changes\n",
    "\n",
    "(may need to restart kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97b0058d-4f24-43e4-af9d-510a5f02247d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from -r translation/requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from -r translation/requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.9/site-packages (from -r translation/requirements.txt (line 3)) (0.1.97)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r translation/requirements.txt (line 4)) (3.20.2)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /opt/conda/lib/python3.9/site-packages (from -r translation/requirements.txt (line 5)) (2.3.1)\n",
      "Requirement already satisfied: py7zr in /opt/conda/lib/python3.9/site-packages (from -r translation/requirements.txt (line 6)) (0.20.4)\n",
      "Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.9/site-packages (from -r translation/requirements.txt (line 7)) (1.13.1+cu117)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.9/site-packages (from -r translation/requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r translation/requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r translation/requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r translation/requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r translation/requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (2.28.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (11.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (2023.1.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (0.13.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (0.70.14)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r translation/requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.9/site-packages (from sacrebleu>=1.4.12->-r translation/requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.9/site-packages (from sacrebleu>=1.4.12->-r translation/requirements.txt (line 5)) (4.9.2)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.9/site-packages (from sacrebleu>=1.4.12->-r translation/requirements.txt (line 5)) (2.7.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.9/site-packages (from sacrebleu>=1.4.12->-r translation/requirements.txt (line 5)) (0.4.4)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.9/site-packages (from sacrebleu>=1.4.12->-r translation/requirements.txt (line 5)) (2022.10.31)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from py7zr->-r translation/requirements.txt (line 6)) (3.17)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from py7zr->-r translation/requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /opt/conda/lib/python3.9/site-packages (from py7zr->-r translation/requirements.txt (line 6)) (0.15.4)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /opt/conda/lib/python3.9/site-packages (from py7zr->-r translation/requirements.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from py7zr->-r translation/requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from py7zr->-r translation/requirements.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /opt/conda/lib/python3.9/site-packages (from py7zr->-r translation/requirements.txt (line 6)) (0.2.3)\n",
      "Requirement already satisfied: texttable in /opt/conda/lib/python3.9/site-packages (from py7zr->-r translation/requirements.txt (line 6)) (1.6.7)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3->-r translation/requirements.txt (line 7)) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (3.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r translation/requirements.txt (line 2)) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Obtaining file:///root/transformers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (0.13.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (23.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.0.dev1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev1) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.0.dev1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.0.dev1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.0.dev1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.0.dev1) (2.1.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building editable for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.27.0.dev1-0.editable-py3-none-any.whl size=34577 sha256=aa1db6ae56fb148650c6fcbb8b85949dff916a7675b4aac7e0276ba561eaeb2d\n",
      "  Stored in directory: /root/transformers/examples/pytorch/pip-ephem-wheel-cache-aeb8ueft/wheels/c6/d4/24/fe65e904e9dddeaa02668d56c7a9bd848371752e768d6bb33b\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.27.0.dev1\n",
      "    Uninstalling transformers-4.27.0.dev1:\n",
      "      Successfully uninstalled transformers-4.27.0.dev1\n",
      "Successfully installed transformers-4.27.0.dev1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r translation/requirements.txt\n",
    "%pip install -e ~/transformers/ #Or wherever you downloaded this source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c394a69-2fa5-4b3a-928b-ea8c8d7e5bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from translation import run_translation\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from unittest.mock import patch\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import ViTMAEForPreTraining, Wav2Vec2ForPreTraining\n",
    "from transformers.testing_utils import CaptureLogger, TestCasePlus, get_gpu_count, slow, torch_device\n",
    "from transformers.utils import is_apex_available\n",
    "from utils import *\n",
    "\n",
    "def get_results(output_dir):\n",
    "    results = {}\n",
    "    path = os.path.join(output_dir, \"all_results.json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"can't find {path}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b803785-a826-44cc-808f-ae4e7c53ab73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a8cb0-9242-4b31-a495-3a68de2dde60",
   "metadata": {},
   "source": [
    "### Default AdamW_HF with LambdaLR (linear, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b193adaa-5083-48cb-993e-2c753f4f447a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/15/2023 16:08:05 - WARNING - translation.run_translation - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/15/2023 16:08:05 - INFO - translation.run_translation - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "factor=0.99,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.01,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/transformers/examples/pytorch/tmp06ou4ile/runs/Mar15_16-08-05_pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=cosine_with_restarts,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10000,\n",
      "metric_for_best_model=None,\n",
      "min_lr=0.001,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adafactor,\n",
      "optim_args=None,\n",
      "output_dir=/root/transformers/examples/pytorch/tmp06ou4ile,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "patience=10,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/transformers/examples/pytorch/tmp06ou4ile,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "smooth=False,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=1000,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/15/2023 16:08:05 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\n",
      "03/15/2023 16:08:05 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/15/2023 16:08:05 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\n",
      "03/15/2023 16:08:05 - WARNING - datasets.builder - Found cached dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\n",
      "03/15/2023 16:08:05 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 62.22it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-03-15 16:08:05,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-15 16:08:05,850 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:668] 2023-03-15 16:08:05,896 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-15 16:08:05,897 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-15 16:08:05,908 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-15 16:08:05,909 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-15 16:08:05,910 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-15 16:08:05,910 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-15 16:08:05,911 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:668] 2023-03-15 16:08:05,915 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-15 16:08:05,916 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:668] 2023-03-15 16:08:06,424 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-15 16:08:06,426 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2398] 2023-03-15 16:08:07,550 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:575] 2023-03-15 16:08:08,262 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.27.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3019] 2023-03-15 16:08:14,335 >> All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3027] 2023-03-15 16:08:14,336 >> All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at facebook/mbart-large-50-many-to-many-mmt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2023-03-15 16:08:14,416 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2023-03-15 16:08:14,417 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.27.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/15/2023 16:08:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227/cache-e89686defc0b1a97.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:543] 2023-03-15 16:08:15,855 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:1745] 2023-03-15 16:08:15,876 >> ***** Running training *****\n",
      "[INFO|trainer.py:1746] 2023-03-15 16:08:15,877 >>   Num examples = 4548885\n",
      "[INFO|trainer.py:1747] 2023-03-15 16:08:15,877 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1748] 2023-03-15 16:08:15,878 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1749] 2023-03-15 16:08:15,878 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1750] 2023-03-15 16:08:15,879 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1751] 2023-03-15 16:08:15,880 >>   Total optimization steps = 10000\n",
      "[INFO|trainer.py:1752] 2023-03-15 16:08:15,881 >>   Number of trainable parameters = 610879488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-15 16:08:16.034: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-03-15 16:08:16.062 pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e:19276 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-03-15 16:08:16.079 pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e:19276 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:280] 2023-03-15 16:08:16,318 >> You're using a MBart50TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 48:28, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>7.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>7.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>7.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>9.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>8.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>9.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>10.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>7.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>7.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>7.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>7.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>7.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>7.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>7.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>6.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>7.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>6.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>7.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>6.836700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>6.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>6.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>6.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>6.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>6.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>6.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>6.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>6.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>6.721600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>6.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>6.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>6.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>6.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>6.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>6.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>6.615200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>6.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>6.588600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>6.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>6.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>6.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.551200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>6.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>6.480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>6.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>6.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>6.502400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>6.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>6.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>6.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>6.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>6.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>6.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>6.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>6.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>6.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>6.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>6.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>6.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>6.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>6.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>6.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>6.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>6.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>6.312400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>6.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>6.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>6.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>6.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>6.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>6.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>6.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>6.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>6.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>6.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>6.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>6.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>6.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>6.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>6.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>6.296100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>6.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>6.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>6.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>6.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>6.297100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>6.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>6.214900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>6.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>6.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>6.169800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2019] 2023-03-15 16:56:46,121 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|modelcard.py:449] 2023-03-15 16:56:46,215 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'wmt16 de-en', 'type': 'wmt16', 'config': 'de-en', 'split': 'train', 'args': 'de-en'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.01\n",
      "  total_flos               =  4615994GF\n",
      "  train_loss               =     6.6881\n",
      "  train_runtime            = 0:48:30.23\n",
      "  train_samples            =    4548885\n",
      "  train_samples_per_second =     13.745\n",
      "  train_steps_per_second   =      3.436\n"
     ]
    }
   ],
   "source": [
    "tmp_dir = get_auto_remove_tmp_dir()\n",
    "testargs = f\"\"\"\n",
    "    run_translation.py\n",
    "    --model_name_or_path facebook/mbart-large-50-many-to-many-mmt\n",
    "    --source_lang de\n",
    "    --target_lang en\n",
    "    --dataset_name wmt16\n",
    "    --dataset_config_name de-en\n",
    "    --output_dir {tmp_dir}\n",
    "    --overwrite_output_dir\n",
    "    --max_steps=10000\n",
    "    --warmup_steps=1000\n",
    "    --do_train\n",
    "    --learning_rate=1e-2\n",
    "    --per_device_train_batch_size=4\n",
    "    --per_device_eval_batch_size=4\n",
    "    --predict_with_generate\n",
    "    --optim adafactor\n",
    "    --lr_scheduler_type cosine_with_restarts\n",
    "    --save_strategy no\n",
    "    --logging_steps 100\n",
    "\"\"\".split()\n",
    "\n",
    "with patch.object(sys, \"argv\", testargs):\n",
    "    run_translation.main()\n",
    "    result = get_results(tmp_dir)\n",
    "    # print(result[\"eval_bleu\"]>30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ca578d-b857-4b0f-9973-77ca99913e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "alllogs = json.load(open(f'{tmp_dir}/trainer_state.json'))\n",
    "d1 = [ (l['step'], l['learning_rate'], l['loss']) for l in alllogs['log_history'][:-1] ]\n",
    "\n",
    "!rm -r {tmp_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ce81ff-c346-4734-8fd0-24c7d89e3e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r d1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7667a-875f-410b-89a7-8c9207fcc745",
   "metadata": {},
   "source": [
    "### With GreedyLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b9170-1e41-441c-b2d1-e6799ab74db0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/16/2023 02:12:50 - WARNING - translation.run_translation - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/16/2023 02:12:50 - INFO - translation.run_translation - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "factor=0.9,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.01,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/transformers/examples/pytorch/tmpdijbn_ga/runs/Mar16_02-12-50_pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=greedy,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10000,\n",
      "metric_for_best_model=None,\n",
      "min_lr=0.001,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adafactor,\n",
      "optim_args=None,\n",
      "output_dir=/root/transformers/examples/pytorch/tmpdijbn_ga,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "patience=10,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/transformers/examples/pytorch/tmpdijbn_ga,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "smooth=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=1000,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/16/2023 02:12:50 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\n",
      "03/16/2023 02:12:50 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/16/2023 02:12:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\n",
      "03/16/2023 02:12:50 - WARNING - datasets.builder - Found cached dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\n",
      "03/16/2023 02:12:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 62.23it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 02:12:50,779 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 02:12:50,797 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 02:12:50,865 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 02:12:50,867 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 02:12:50,878 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 02:12:50,879 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 02:12:50,879 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 02:12:50,880 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 02:12:50,880 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 02:12:50,884 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 02:12:50,885 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 02:12:51,400 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 02:12:51,401 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2398] 2023-03-16 02:12:52,532 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:575] 2023-03-16 02:12:53,245 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.27.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3019] 2023-03-16 02:12:59,275 >> All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3027] 2023-03-16 02:12:59,276 >> All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at facebook/mbart-large-50-many-to-many-mmt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2023-03-16 02:12:59,354 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50-many-to-many-mmt/snapshots/e2cfb9f4d0cfb8879734094041a08c37397b3177/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2023-03-16 02:12:59,355 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.27.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/16/2023 02:12:59 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227/cache-e89686defc0b1a97.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:543] 2023-03-16 02:13:00,817 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:1745] 2023-03-16 02:13:00,838 >> ***** Running training *****\n",
      "[INFO|trainer.py:1746] 2023-03-16 02:13:00,839 >>   Num examples = 4548885\n",
      "[INFO|trainer.py:1747] 2023-03-16 02:13:00,839 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1748] 2023-03-16 02:13:00,840 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1749] 2023-03-16 02:13:00,840 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1750] 2023-03-16 02:13:00,841 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1751] 2023-03-16 02:13:00,841 >>   Total optimization steps = 10000\n",
      "[INFO|trainer.py:1752] 2023-03-16 02:13:00,843 >>   Number of trainable parameters = 610879488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedyLR settings: patience=10 smooth=True min_lr=0.001 factor=0.9\n",
      "[2023-03-16 02:13:00.996: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-03-16 02:13:01.025 pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e:20806 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-03-16 02:13:01.042 pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e:20806 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:280] 2023-03-16 02:13:01,276 >> You're using a MBart50TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='473' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  473/10000 02:18 < 46:41, 3.40 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_dir = get_auto_remove_tmp_dir()\n",
    "testargs = f\"\"\"\n",
    "    run_translation.py\n",
    "    --model_name_or_path facebook/mbart-large-50-many-to-many-mmt\n",
    "    --source_lang de\n",
    "    --target_lang en\n",
    "    --dataset_name wmt16\n",
    "    --dataset_config_name de-en\n",
    "    --output_dir {tmp_dir}\n",
    "    --overwrite_output_dir\n",
    "    --max_steps=10000\n",
    "    --warmup_steps=0\n",
    "    --do_train\n",
    "    --learning_rate=1e-2\n",
    "    --per_device_train_batch_size=4\n",
    "    --per_device_eval_batch_size=4\n",
    "    --predict_with_generate\n",
    "    --optim adafactor\n",
    "    --lr_scheduler_type greedy\n",
    "    --save_strategy no\n",
    "    --logging_steps 500\n",
    "    --min_lr=1e-3\n",
    "    --smooth True\n",
    "    --patience 10\n",
    "    --factor 0.9\n",
    "\"\"\".split()\n",
    "\n",
    "with patch.object(sys, \"argv\", testargs):\n",
    "    run_translation.main()\n",
    "    result = get_results(tmp_dir)\n",
    "    # print(result[\"eval_bleu\"]>30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca37c9e-a80a-40e8-982f-319a088edad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alllogs = json.load(open(f'{tmp_dir}/trainer_state.json'))\n",
    "d2 = [ (l['step'], l['learning_rate'], l['loss']) for l in alllogs['log_history'][:-1] ]\n",
    "\n",
    "!rm -r {tmp_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "783d4f6b-5eda-4e06-89c4-1d608d2e886e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "696dc61a-178f-4213-98c9-130d03d5b38e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'LRs')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = np.array(d1)[:,0]\n",
    "y1 = np.array(d1)[:,1]\n",
    "\n",
    "x2 = np.array(d2)[:,0] \n",
    "y2 = np.array(d2)[:,1]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "line1, = ax.plot(x1, y1, 'b-', label='baseline')\n",
    "line2, = ax.plot(x2, y2, 'r-', label='GreedyLR')\n",
    "plt.legend()\n",
    "plt.title('LRs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aafae40a-f887-47ab-8f5e-1948128b8d72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = np.array(d1)[:,0]\n",
    "y1 = np.array(d1)[:,2]\n",
    "\n",
    "x2 = np.array(d2)[:,0] \n",
    "y2 = np.array(d2)[:,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "line1, = ax.plot(x1, y1, 'b-',label='baseline')\n",
    "line2, = ax.plot(x2, y2, 'r-',label='GreedyLR')\n",
    "plt.legend()\n",
    "plt.title('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c195ead-5cf7-46ac-899b-02bf7b4c75ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32810000000000006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1[-1][-1] - d2[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902b59e-efc2-4f29-b93b-ea0fca48b979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c77c7e-75eb-4946-8fa9-1665c89e96ca",
   "metadata": {},
   "source": [
    "## 2. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828578e8-2fff-4bf6-bda1-fb1de61e66ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from -r question_answering/requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from -r question_answering/requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.9/site-packages (from -r question_answering/requirements.txt (line 3)) (1.13.1+cu117)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.9/site-packages (from -r question_answering/requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r question_answering/requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r question_answering/requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r question_answering/requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r question_answering/requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (3.8.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (2023.1.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (0.18.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (0.13.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (11.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3.0->-r question_answering/requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r question_answering/requirements.txt (line 2)) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Obtaining file:///root/transformers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\\"
     ]
    }
   ],
   "source": [
    "%pip install -r question_answering/requirements.txt\n",
    "%pip install -e ~/transformers/ #Or wherever you downloaded this source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350de744-e910-4bd1-86f8-b4e388d58a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from question_answering import run_qa as run_squad\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from unittest.mock import patch\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import ViTMAEForPreTraining, Wav2Vec2ForPreTraining\n",
    "from transformers.testing_utils import CaptureLogger, TestCasePlus, get_gpu_count, slow, torch_device\n",
    "from transformers.utils import is_apex_available\n",
    "from utils import *\n",
    "\n",
    "def get_results(output_dir):\n",
    "    results = {}\n",
    "    path = os.path.join(output_dir, \"all_results.json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"can't find {path}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e736f06-d6e3-46af-a6e4-317ff5623090",
   "metadata": {},
   "source": [
    "### Default ADAMHF / Adafactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8317c07-cd7b-4130-a7ca-af5448577575",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/16/2023 18:46:28 - WARNING - question_answering.run_qa - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/16/2023 18:46:28 - INFO - question_answering.run_qa - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "factor=0.99,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/transformers/examples/pytorch/tmp2o0592r7/runs/Mar16_18-46-28_pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=cosine_with_restarts,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10000,\n",
      "metric_for_best_model=None,\n",
      "min_lr=0.001,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adafactor,\n",
      "optim_args=None,\n",
      "output_dir=/root/transformers/examples/pytorch/tmp2o0592r7,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "patience=10,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/transformers/examples/pytorch/tmp2o0592r7,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "smooth=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=1000,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/16/2023 18:46:29 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/adversarial_qa/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b\n",
      "03/16/2023 18:46:29 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/16/2023 18:46:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b\n",
      "03/16/2023 18:46:29 - WARNING - datasets.builder - Found cached dataset adversarial_qa (/root/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n",
      "03/16/2023 18:46:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 424.94it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 18:46:29,408 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 18:46:29,426 >> Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:478] 2023-03-16 18:46:29,475 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 18:46:29,515 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 18:46:29,516 >> Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 18:46:29,613 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 18:46:29,614 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 18:46:29,614 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 18:46:29,615 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 18:46:29,615 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 18:46:29,619 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 18:46:29,620 >> Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2398] 2023-03-16 18:46:30,283 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:3009] 2023-03-16 18:46:32,430 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3021] 2023-03-16 18:46:32,431 >> Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/16/2023 18:46:32 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b/cache-5f2c62420c8bd2ee.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:543] 2023-03-16 18:46:33,801 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:1745] 2023-03-16 18:46:33,809 >> ***** Running training *****\n",
      "[INFO|trainer.py:1746] 2023-03-16 18:46:33,810 >>   Num examples = 30833\n",
      "[INFO|trainer.py:1747] 2023-03-16 18:46:33,810 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1748] 2023-03-16 18:46:33,811 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1749] 2023-03-16 18:46:33,811 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:1750] 2023-03-16 18:46:33,812 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1751] 2023-03-16 18:46:33,812 >>   Total optimization steps = 10000\n",
      "[INFO|trainer.py:1752] 2023-03-16 18:46:33,813 >>   Number of trainable parameters = 277454594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-16 18:46:33.963: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-03-16 18:46:33.991 pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e:23797 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-03-16 18:46:34.015 pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e:23797 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 21:34, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>6.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>6.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>6.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>6.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>6.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>6.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>6.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>6.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>6.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>6.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>6.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>6.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>6.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>6.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>6.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>6.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>5.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>6.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>6.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>5.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>5.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>5.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>5.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>6.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>5.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>5.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>5.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>6.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>5.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>5.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>5.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.960200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>5.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>5.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>5.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>5.954300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>5.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>5.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>5.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>5.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>5.968800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>5.967300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>5.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>5.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>5.962900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>5.955700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>5.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>5.953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>5.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>5.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>5.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>5.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>5.952800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>5.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.954300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>5.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>5.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>5.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>5.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>5.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>5.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>5.954300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>5.949300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>5.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>5.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>5.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>5.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>5.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>5.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>5.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>5.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>5.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>5.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>5.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>5.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>5.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>5.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>5.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>5.962700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.951100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2019] 2023-03-16 19:08:09,510 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2821] 2023-03-16 19:08:09,519 >> Saving model checkpoint to /root/transformers/examples/pytorch/tmp2o0592r7\n",
      "[INFO|configuration_utils.py:457] 2023-03-16 19:08:09,529 >> Configuration saved in /root/transformers/examples/pytorch/tmp2o0592r7/config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-03-16 19:08:21,101 >> Model weights saved in /root/transformers/examples/pytorch/tmp2o0592r7/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-03-16 19:08:21,112 >> tokenizer config file saved in /root/transformers/examples/pytorch/tmp2o0592r7/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-03-16 19:08:21,120 >> Special tokens file saved in /root/transformers/examples/pytorch/tmp2o0592r7/special_tokens_map.json\n",
      "[INFO|modelcard.py:449] 2023-03-16 19:08:21,731 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'adversarial_qa adversarialQA', 'type': 'adversarial_qa', 'config': 'adversarialQA', 'split': 'train', 'args': 'adversarialQA'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.65\n",
      "  total_flos               =  3650273GF\n",
      "  train_loss               =     5.9803\n",
      "  train_runtime            = 0:21:35.69\n",
      "  train_samples            =      30833\n",
      "  train_samples_per_second =     15.436\n",
      "  train_steps_per_second   =      7.718\n"
     ]
    }
   ],
   "source": [
    "tmp_dir = get_auto_remove_tmp_dir()\n",
    "testargs = f\"\"\"\n",
    "    run_qa.py\n",
    "    --model_name_or_path xlm-roberta-base\n",
    "    --version_2_with_negative\n",
    "    --dataset_name adversarial_qa\n",
    "    --dataset_config adversarialQA\n",
    "    --output_dir {tmp_dir}\n",
    "    --overwrite_output_dir\n",
    "    --max_steps=10000\n",
    "    --warmup_steps=1000\n",
    "    --do_train\n",
    "    --learning_rate=5e-3\n",
    "    --per_device_train_batch_size=2\n",
    "    --per_device_eval_batch_size=1\n",
    "    --save_strategy no\n",
    "    --logging_steps 100\n",
    "    --lr_scheduler_type cosine_with_restarts\n",
    "    --optim adafactor\n",
    "\"\"\".split()\n",
    "\n",
    "with patch.object(sys, \"argv\", testargs):\n",
    "    run_squad.main()\n",
    "    result = get_results(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c5ed56-53bc-47fc-b58e-933d60d304ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "alllogs = json.load(open(f'{tmp_dir}/trainer_state.json'))\n",
    "d1 = [ (l['step'], l['learning_rate'], l['loss']) for l in alllogs['log_history'][:-1] ]\n",
    "\n",
    "!rm -r {tmp_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b40af-44d6-44fb-9880-338c3b01ac17",
   "metadata": {},
   "source": [
    "### Greedy LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2fabf8-33f9-497a-a361-2597cd1b5de4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1509] 2023-03-16 19:08:22,234 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1289] 2023-03-16 19:08:22,235 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/16/2023 19:08:22 - WARNING - question_answering.run_qa - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/16/2023 19:08:22 - INFO - question_answering.run_qa - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "factor=0.9,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/transformers/examples/pytorch/tmpdpkpzar3/runs/Mar16_19-08-22_pytorch-1-13-gpu-py-ml-g5-16xlarge-bff23aa7fcca0c0ba1c19f2e4a3e,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=greedy,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10000,\n",
      "metric_for_best_model=None,\n",
      "min_lr=1e-05,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adafactor,\n",
      "optim_args=None,\n",
      "output_dir=/root/transformers/examples/pytorch/tmpdpkpzar3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "patience=10,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/transformers/examples/pytorch/tmpdpkpzar3,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "smooth=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/16/2023 19:08:22 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/adversarial_qa/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b\n",
      "03/16/2023 19:08:22 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/16/2023 19:08:22 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b\n",
      "03/16/2023 19:08:22 - WARNING - datasets.builder - Found cached dataset adversarial_qa (/root/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n",
      "03/16/2023 19:08:22 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 503.86it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 19:08:22,709 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 19:08:22,711 >> Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:478] 2023-03-16 19:08:22,757 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 19:08:22,797 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 19:08:22,798 >> Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 19:08:22,890 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 19:08:22,891 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 19:08:22,891 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 19:08:22,892 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-03-16 19:08:22,892 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2023-03-16 19:08:22,896 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-03-16 19:08:22,897 >> Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2398] 2023-03-16 19:08:23,356 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:3009] 2023-03-16 19:08:25,465 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3021] 2023-03-16 19:08:25,466 >> Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/16/2023 19:08:25 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b/cache-5f2c62420c8bd2ee.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:543] 2023-03-16 19:08:25,823 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:1745] 2023-03-16 19:08:25,833 >> ***** Running training *****\n",
      "[INFO|trainer.py:1746] 2023-03-16 19:08:25,834 >>   Num examples = 30833\n",
      "[INFO|trainer.py:1747] 2023-03-16 19:08:25,835 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1748] 2023-03-16 19:08:25,835 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1749] 2023-03-16 19:08:25,836 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1750] 2023-03-16 19:08:25,836 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1751] 2023-03-16 19:08:25,836 >>   Total optimization steps = 10000\n",
      "[INFO|trainer.py:1752] 2023-03-16 19:08:25,837 >>   Number of trainable parameters = 277454594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedyLR settings: patience=10 smooth=True min_lr=1e-05 factor=0.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 27:22, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>6.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>6.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>6.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>6.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.974200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>6.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>6.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>6.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>5.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>5.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>6.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>6.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>6.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>5.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>6.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>6.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>5.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>5.962900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>6.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>6.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>6.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>5.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>6.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>5.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>5.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>5.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>5.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.986600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>6.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>5.998500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>6.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>5.981600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>6.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>6.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>6.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>5.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>5.981100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>6.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>5.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>5.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>6.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.981600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>5.983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>5.981800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>5.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>6.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>5.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>5.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>6.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>6.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>5.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>6.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>5.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>5.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>5.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>5.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>6.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>5.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>6.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>5.910500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>5.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>5.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>6.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>5.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>5.929100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>5.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>5.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>5.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>6.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>6.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>5.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>5.995400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>5.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>5.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>5.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>5.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>6.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>5.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>6.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>5.987200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>5.980300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>5.921300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>5.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>6.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>5.962900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>6.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>6.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>5.958900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>6.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>5.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>6.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>5.947400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>5.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>6.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>5.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>5.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>5.947800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>5.934300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>6.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>6.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>5.940500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>5.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>5.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>6.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>5.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>5.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>5.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>6.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>5.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>6.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>5.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>6.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>5.954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>6.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>5.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>5.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>6.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>5.958500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>6.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>6.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>5.937900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>5.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>5.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>5.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>5.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>6.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>5.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>5.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>5.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>6.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>6.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>5.951300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>6.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>5.961600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>5.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>5.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>5.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>6.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>6.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>5.957600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>6.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>5.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>5.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>6.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>5.962700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>5.976400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>6.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>5.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>5.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>5.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>5.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>5.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>6.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>5.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>5.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>5.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>5.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>5.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>5.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>5.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>5.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>6.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>5.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>5.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>5.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>5.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>6.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>6.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>5.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>6.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>5.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>6.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>5.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>6.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>5.946900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>5.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>5.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>5.965400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>6.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>5.894600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>5.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>5.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>5.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>5.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>6.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>5.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>6.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>5.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>6.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>5.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>6.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>6.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>6.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>5.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>6.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>6.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>5.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>5.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>5.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>5.951400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>5.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>5.933700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>5.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>5.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>6.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>6.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>5.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>5.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>5.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>5.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>5.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>6.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>6.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>6.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>6.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>6.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>5.932400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>6.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>5.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>5.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>5.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>5.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>6.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>5.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>5.968500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>5.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>6.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>5.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>5.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>5.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>6.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>5.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>5.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>6.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>5.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>5.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>6.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>5.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>6.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>5.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>5.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>6.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>5.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>6.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>5.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>5.953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>5.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>6.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>5.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>5.977300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>5.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>5.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>5.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>5.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>5.927900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>6.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>6.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>5.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>6.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>6.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>6.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>6.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>5.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>5.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>6.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>5.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>5.966600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>5.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>5.946900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>5.943500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>5.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>6.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>5.971600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>5.977300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>6.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>6.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>5.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>5.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>6.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>5.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>5.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>6.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>5.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>5.950600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>5.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>5.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>5.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>5.926100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>6.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>5.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>5.989400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>5.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>6.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>6.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>5.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>5.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>6.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>5.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>5.962900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>5.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>5.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>6.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>6.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>5.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>6.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>5.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>6.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>5.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>5.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>5.973600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>5.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>5.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>5.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>6.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>5.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>5.991500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>5.995600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>6.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>5.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>5.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>5.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>5.921100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>5.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>5.984500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>6.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>5.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>5.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>6.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>6.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>5.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>5.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>5.995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>6.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>6.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>6.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>5.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>5.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>5.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3630</td>\n",
       "      <td>5.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>6.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>5.987100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>5.969900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>5.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>6.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>6.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>5.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3710</td>\n",
       "      <td>6.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>5.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>5.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>6.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>5.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>5.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>5.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>5.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3790</td>\n",
       "      <td>5.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>5.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3810</td>\n",
       "      <td>5.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>6.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>6.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>5.966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>5.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>5.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>6.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>6.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3890</td>\n",
       "      <td>5.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>5.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>5.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>6.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>6.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>5.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>6.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>5.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3970</td>\n",
       "      <td>5.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>5.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>5.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4010</td>\n",
       "      <td>5.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>5.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>5.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>5.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>5.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>6.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4070</td>\n",
       "      <td>5.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>6.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4090</td>\n",
       "      <td>5.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>6.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4110</td>\n",
       "      <td>6.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>5.978300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4130</td>\n",
       "      <td>5.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>5.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>5.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>5.984800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170</td>\n",
       "      <td>5.930800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>5.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4190</td>\n",
       "      <td>5.987200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>5.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>5.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>5.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>5.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>6.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>5.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>6.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4270</td>\n",
       "      <td>6.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>6.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>6.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>5.937600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4310</td>\n",
       "      <td>5.991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>5.974200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4330</td>\n",
       "      <td>5.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>5.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>5.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>6.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4370</td>\n",
       "      <td>6.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>6.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4390</td>\n",
       "      <td>5.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>5.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>5.964200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>5.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4430</td>\n",
       "      <td>5.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>5.964400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>6.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>5.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4470</td>\n",
       "      <td>5.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>5.981600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4490</td>\n",
       "      <td>5.966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4510</td>\n",
       "      <td>5.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>5.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4530</td>\n",
       "      <td>6.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>6.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>5.984500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>6.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4570</td>\n",
       "      <td>5.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>6.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4590</td>\n",
       "      <td>6.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>5.976400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4610</td>\n",
       "      <td>6.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>5.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4630</td>\n",
       "      <td>6.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>5.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>6.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>5.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4670</td>\n",
       "      <td>5.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>5.997800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4690</td>\n",
       "      <td>5.964200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>5.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4710</td>\n",
       "      <td>5.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>6.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4730</td>\n",
       "      <td>5.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>5.978600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>5.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>5.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4770</td>\n",
       "      <td>5.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>5.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>6.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>5.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4810</td>\n",
       "      <td>5.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>5.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4830</td>\n",
       "      <td>5.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>5.955500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>5.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>5.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4870</td>\n",
       "      <td>6.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>6.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4890</td>\n",
       "      <td>5.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>5.993100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4910</td>\n",
       "      <td>5.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>5.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4930</td>\n",
       "      <td>5.995400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>6.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>5.968500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>5.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4970</td>\n",
       "      <td>5.952600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>6.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4990</td>\n",
       "      <td>6.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.929500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5010</td>\n",
       "      <td>5.955500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>6.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5030</td>\n",
       "      <td>5.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>5.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>6.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>5.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5070</td>\n",
       "      <td>5.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>6.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5090</td>\n",
       "      <td>5.950400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>5.997100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5110</td>\n",
       "      <td>5.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>6.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5130</td>\n",
       "      <td>5.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>5.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>5.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>5.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5170</td>\n",
       "      <td>5.920200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>6.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5190</td>\n",
       "      <td>5.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>6.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5210</td>\n",
       "      <td>5.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>6.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5230</td>\n",
       "      <td>5.922200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>5.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>5.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>6.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5270</td>\n",
       "      <td>6.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>5.993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5290</td>\n",
       "      <td>5.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>6.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5310</td>\n",
       "      <td>5.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>5.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5330</td>\n",
       "      <td>5.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>5.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>6.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>5.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5370</td>\n",
       "      <td>5.940200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>6.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5390</td>\n",
       "      <td>5.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>5.939200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5410</td>\n",
       "      <td>6.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>6.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5430</td>\n",
       "      <td>6.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>5.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>5.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>5.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5470</td>\n",
       "      <td>5.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>6.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5490</td>\n",
       "      <td>6.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5510</td>\n",
       "      <td>5.958800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>6.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5530</td>\n",
       "      <td>5.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>5.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>5.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>5.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>6.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>5.925800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5590</td>\n",
       "      <td>5.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>6.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5610</td>\n",
       "      <td>6.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>5.964200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5630</td>\n",
       "      <td>5.939500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>5.983400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>6.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>5.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5670</td>\n",
       "      <td>6.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>5.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5690</td>\n",
       "      <td>5.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>6.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5710</td>\n",
       "      <td>5.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>6.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5730</td>\n",
       "      <td>5.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>5.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>5.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>5.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5770</td>\n",
       "      <td>6.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>5.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5790</td>\n",
       "      <td>5.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>5.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5810</td>\n",
       "      <td>5.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>5.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830</td>\n",
       "      <td>5.973600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>5.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>5.984800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>5.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5870</td>\n",
       "      <td>6.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>6.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5890</td>\n",
       "      <td>6.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>5.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5910</td>\n",
       "      <td>5.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>5.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5930</td>\n",
       "      <td>5.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>5.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>5.967300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>6.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5970</td>\n",
       "      <td>6.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>5.923400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5990</td>\n",
       "      <td>5.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6010</td>\n",
       "      <td>5.991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>5.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6030</td>\n",
       "      <td>5.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6040</td>\n",
       "      <td>5.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>6.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6060</td>\n",
       "      <td>5.992400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6070</td>\n",
       "      <td>6.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6080</td>\n",
       "      <td>5.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6090</td>\n",
       "      <td>5.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>5.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6110</td>\n",
       "      <td>5.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6120</td>\n",
       "      <td>5.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6130</td>\n",
       "      <td>5.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6140</td>\n",
       "      <td>5.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>5.998500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>6.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6170</td>\n",
       "      <td>5.983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6180</td>\n",
       "      <td>5.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6190</td>\n",
       "      <td>5.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>5.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6210</td>\n",
       "      <td>6.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6220</td>\n",
       "      <td>5.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6230</td>\n",
       "      <td>5.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>6.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>6.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6260</td>\n",
       "      <td>6.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6270</td>\n",
       "      <td>5.961400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6280</td>\n",
       "      <td>6.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6290</td>\n",
       "      <td>6.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>5.953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6310</td>\n",
       "      <td>6.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>5.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6330</td>\n",
       "      <td>5.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6340</td>\n",
       "      <td>5.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>5.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6360</td>\n",
       "      <td>5.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6370</td>\n",
       "      <td>5.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>6.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6390</td>\n",
       "      <td>5.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>5.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6410</td>\n",
       "      <td>6.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6420</td>\n",
       "      <td>5.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6430</td>\n",
       "      <td>5.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6440</td>\n",
       "      <td>5.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>5.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6460</td>\n",
       "      <td>5.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6470</td>\n",
       "      <td>5.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6480</td>\n",
       "      <td>5.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6490</td>\n",
       "      <td>6.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>6.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6510</td>\n",
       "      <td>5.958300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6520</td>\n",
       "      <td>6.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6530</td>\n",
       "      <td>6.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>5.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>6.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6560</td>\n",
       "      <td>6.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6570</td>\n",
       "      <td>5.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6580</td>\n",
       "      <td>5.950400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6590</td>\n",
       "      <td>6.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>5.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6610</td>\n",
       "      <td>5.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6620</td>\n",
       "      <td>5.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6630</td>\n",
       "      <td>6.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>5.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>6.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6660</td>\n",
       "      <td>5.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6670</td>\n",
       "      <td>5.948100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6680</td>\n",
       "      <td>5.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6690</td>\n",
       "      <td>5.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>5.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6710</td>\n",
       "      <td>5.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6720</td>\n",
       "      <td>6.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6730</td>\n",
       "      <td>6.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6740</td>\n",
       "      <td>5.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>5.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6760</td>\n",
       "      <td>5.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6770</td>\n",
       "      <td>5.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6780</td>\n",
       "      <td>6.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6790</td>\n",
       "      <td>5.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>5.992400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6810</td>\n",
       "      <td>5.989400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6820</td>\n",
       "      <td>5.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6830</td>\n",
       "      <td>5.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6840</td>\n",
       "      <td>5.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>6.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6860</td>\n",
       "      <td>5.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6870</td>\n",
       "      <td>5.981600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6880</td>\n",
       "      <td>5.931700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6890</td>\n",
       "      <td>5.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>5.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6910</td>\n",
       "      <td>6.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6920</td>\n",
       "      <td>5.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6930</td>\n",
       "      <td>5.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6940</td>\n",
       "      <td>5.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>6.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6960</td>\n",
       "      <td>5.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6970</td>\n",
       "      <td>6.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6980</td>\n",
       "      <td>5.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6990</td>\n",
       "      <td>5.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7010</td>\n",
       "      <td>6.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>5.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7030</td>\n",
       "      <td>6.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7040</td>\n",
       "      <td>5.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>5.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7060</td>\n",
       "      <td>6.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7070</td>\n",
       "      <td>5.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7080</td>\n",
       "      <td>6.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7090</td>\n",
       "      <td>5.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>5.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7110</td>\n",
       "      <td>5.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7120</td>\n",
       "      <td>6.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7130</td>\n",
       "      <td>5.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7140</td>\n",
       "      <td>5.946600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>6.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7160</td>\n",
       "      <td>5.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7170</td>\n",
       "      <td>5.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7180</td>\n",
       "      <td>5.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7190</td>\n",
       "      <td>5.964300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>6.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7210</td>\n",
       "      <td>6.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7220</td>\n",
       "      <td>5.963400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7230</td>\n",
       "      <td>6.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7240</td>\n",
       "      <td>5.984800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>5.996900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7260</td>\n",
       "      <td>5.963600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7270</td>\n",
       "      <td>5.976900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7280</td>\n",
       "      <td>6.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7290</td>\n",
       "      <td>5.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>5.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7310</td>\n",
       "      <td>5.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7320</td>\n",
       "      <td>5.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7330</td>\n",
       "      <td>5.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7340</td>\n",
       "      <td>5.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>5.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7360</td>\n",
       "      <td>6.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7370</td>\n",
       "      <td>5.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7380</td>\n",
       "      <td>6.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7390</td>\n",
       "      <td>5.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>5.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7410</td>\n",
       "      <td>5.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7420</td>\n",
       "      <td>5.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7430</td>\n",
       "      <td>6.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7440</td>\n",
       "      <td>5.955500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>5.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7460</td>\n",
       "      <td>5.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7470</td>\n",
       "      <td>5.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7480</td>\n",
       "      <td>5.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7490</td>\n",
       "      <td>5.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7510</td>\n",
       "      <td>6.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7520</td>\n",
       "      <td>5.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7530</td>\n",
       "      <td>5.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7540</td>\n",
       "      <td>6.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>5.935100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7560</td>\n",
       "      <td>5.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7570</td>\n",
       "      <td>5.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7580</td>\n",
       "      <td>5.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7590</td>\n",
       "      <td>6.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>6.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7610</td>\n",
       "      <td>6.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7620</td>\n",
       "      <td>5.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7630</td>\n",
       "      <td>5.982100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7640</td>\n",
       "      <td>6.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>6.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7660</td>\n",
       "      <td>5.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7670</td>\n",
       "      <td>5.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7680</td>\n",
       "      <td>5.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7690</td>\n",
       "      <td>5.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>5.920800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7710</td>\n",
       "      <td>5.970100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7720</td>\n",
       "      <td>5.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7730</td>\n",
       "      <td>5.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7740</td>\n",
       "      <td>5.998500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>5.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7760</td>\n",
       "      <td>5.954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7770</td>\n",
       "      <td>5.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7780</td>\n",
       "      <td>6.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7790</td>\n",
       "      <td>6.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>5.945300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7810</td>\n",
       "      <td>5.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7820</td>\n",
       "      <td>5.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7830</td>\n",
       "      <td>5.984500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7840</td>\n",
       "      <td>5.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>5.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7860</td>\n",
       "      <td>5.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7870</td>\n",
       "      <td>5.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7880</td>\n",
       "      <td>5.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7890</td>\n",
       "      <td>5.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>5.991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7910</td>\n",
       "      <td>5.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7920</td>\n",
       "      <td>5.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7930</td>\n",
       "      <td>5.943200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7940</td>\n",
       "      <td>5.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>5.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7960</td>\n",
       "      <td>5.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7970</td>\n",
       "      <td>5.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7980</td>\n",
       "      <td>5.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7990</td>\n",
       "      <td>5.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8010</td>\n",
       "      <td>6.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8020</td>\n",
       "      <td>6.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8030</td>\n",
       "      <td>5.965500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8040</td>\n",
       "      <td>5.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>6.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8060</td>\n",
       "      <td>5.974200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8070</td>\n",
       "      <td>5.940500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8080</td>\n",
       "      <td>6.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8090</td>\n",
       "      <td>5.964300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>5.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8110</td>\n",
       "      <td>6.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8120</td>\n",
       "      <td>6.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8130</td>\n",
       "      <td>5.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8140</td>\n",
       "      <td>6.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>6.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8160</td>\n",
       "      <td>5.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8170</td>\n",
       "      <td>5.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8180</td>\n",
       "      <td>5.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8190</td>\n",
       "      <td>6.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>5.983400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8210</td>\n",
       "      <td>5.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8220</td>\n",
       "      <td>5.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8230</td>\n",
       "      <td>5.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8240</td>\n",
       "      <td>5.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>5.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8260</td>\n",
       "      <td>5.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8270</td>\n",
       "      <td>5.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8280</td>\n",
       "      <td>6.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8290</td>\n",
       "      <td>5.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>5.993900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8310</td>\n",
       "      <td>5.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8320</td>\n",
       "      <td>5.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8330</td>\n",
       "      <td>5.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8340</td>\n",
       "      <td>5.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>5.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8360</td>\n",
       "      <td>5.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8370</td>\n",
       "      <td>5.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8380</td>\n",
       "      <td>5.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8390</td>\n",
       "      <td>5.923200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>5.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8410</td>\n",
       "      <td>6.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8420</td>\n",
       "      <td>5.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8430</td>\n",
       "      <td>5.925100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8440</td>\n",
       "      <td>6.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>5.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8460</td>\n",
       "      <td>5.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8470</td>\n",
       "      <td>5.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8480</td>\n",
       "      <td>6.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8490</td>\n",
       "      <td>5.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.951300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8510</td>\n",
       "      <td>5.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8520</td>\n",
       "      <td>5.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8530</td>\n",
       "      <td>5.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8540</td>\n",
       "      <td>5.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>6.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8560</td>\n",
       "      <td>5.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8570</td>\n",
       "      <td>5.944400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8580</td>\n",
       "      <td>6.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8590</td>\n",
       "      <td>5.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>5.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8610</td>\n",
       "      <td>5.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8620</td>\n",
       "      <td>6.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8630</td>\n",
       "      <td>5.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8640</td>\n",
       "      <td>5.982100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>5.973600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8660</td>\n",
       "      <td>5.995600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8670</td>\n",
       "      <td>5.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8680</td>\n",
       "      <td>5.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8690</td>\n",
       "      <td>5.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>5.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8710</td>\n",
       "      <td>5.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8720</td>\n",
       "      <td>5.966600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8730</td>\n",
       "      <td>6.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8740</td>\n",
       "      <td>5.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>5.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8760</td>\n",
       "      <td>5.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8770</td>\n",
       "      <td>5.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8780</td>\n",
       "      <td>5.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8790</td>\n",
       "      <td>5.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>5.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8810</td>\n",
       "      <td>5.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8820</td>\n",
       "      <td>6.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8830</td>\n",
       "      <td>5.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8840</td>\n",
       "      <td>6.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>5.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8860</td>\n",
       "      <td>6.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8870</td>\n",
       "      <td>5.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8880</td>\n",
       "      <td>5.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8890</td>\n",
       "      <td>5.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>5.946900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8910</td>\n",
       "      <td>5.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8920</td>\n",
       "      <td>5.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8930</td>\n",
       "      <td>5.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8940</td>\n",
       "      <td>5.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>5.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8960</td>\n",
       "      <td>5.970700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8970</td>\n",
       "      <td>5.992400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8980</td>\n",
       "      <td>5.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8990</td>\n",
       "      <td>5.968800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>6.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9010</td>\n",
       "      <td>5.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9020</td>\n",
       "      <td>6.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9030</td>\n",
       "      <td>5.975300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9040</td>\n",
       "      <td>5.940400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>5.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9060</td>\n",
       "      <td>5.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9070</td>\n",
       "      <td>6.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9080</td>\n",
       "      <td>5.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9090</td>\n",
       "      <td>5.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>6.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9110</td>\n",
       "      <td>5.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9120</td>\n",
       "      <td>5.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9130</td>\n",
       "      <td>6.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9140</td>\n",
       "      <td>6.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>5.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9160</td>\n",
       "      <td>6.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9170</td>\n",
       "      <td>5.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9180</td>\n",
       "      <td>6.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9190</td>\n",
       "      <td>6.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>5.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9210</td>\n",
       "      <td>5.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9220</td>\n",
       "      <td>6.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9230</td>\n",
       "      <td>6.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9240</td>\n",
       "      <td>5.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>5.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9260</td>\n",
       "      <td>5.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9270</td>\n",
       "      <td>5.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9280</td>\n",
       "      <td>6.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9290</td>\n",
       "      <td>5.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>5.978300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9310</td>\n",
       "      <td>5.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9320</td>\n",
       "      <td>5.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9330</td>\n",
       "      <td>6.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9340</td>\n",
       "      <td>5.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>5.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9360</td>\n",
       "      <td>5.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9370</td>\n",
       "      <td>5.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9380</td>\n",
       "      <td>5.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9390</td>\n",
       "      <td>5.983400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>6.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9410</td>\n",
       "      <td>5.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9420</td>\n",
       "      <td>6.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9430</td>\n",
       "      <td>6.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9440</td>\n",
       "      <td>5.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>5.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9460</td>\n",
       "      <td>5.965400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9470</td>\n",
       "      <td>5.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9480</td>\n",
       "      <td>5.958500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9490</td>\n",
       "      <td>5.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9510</td>\n",
       "      <td>5.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9520</td>\n",
       "      <td>6.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9530</td>\n",
       "      <td>5.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9540</td>\n",
       "      <td>5.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>5.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9560</td>\n",
       "      <td>5.975300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9570</td>\n",
       "      <td>5.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9580</td>\n",
       "      <td>5.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9590</td>\n",
       "      <td>5.961700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>6.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9610</td>\n",
       "      <td>5.975500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9620</td>\n",
       "      <td>5.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9630</td>\n",
       "      <td>6.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9640</td>\n",
       "      <td>5.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>5.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9660</td>\n",
       "      <td>6.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9670</td>\n",
       "      <td>5.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9680</td>\n",
       "      <td>6.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9690</td>\n",
       "      <td>5.956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>5.981800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9710</td>\n",
       "      <td>6.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9720</td>\n",
       "      <td>5.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9730</td>\n",
       "      <td>6.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9740</td>\n",
       "      <td>5.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>5.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9760</td>\n",
       "      <td>5.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9770</td>\n",
       "      <td>6.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9780</td>\n",
       "      <td>5.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9790</td>\n",
       "      <td>5.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>5.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9810</td>\n",
       "      <td>5.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9820</td>\n",
       "      <td>5.959400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9830</td>\n",
       "      <td>5.973200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9840</td>\n",
       "      <td>5.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>6.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9860</td>\n",
       "      <td>5.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9870</td>\n",
       "      <td>5.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9880</td>\n",
       "      <td>5.921200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9890</td>\n",
       "      <td>6.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>5.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9910</td>\n",
       "      <td>5.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9920</td>\n",
       "      <td>5.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9930</td>\n",
       "      <td>6.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9940</td>\n",
       "      <td>5.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>5.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9960</td>\n",
       "      <td>5.938700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9970</td>\n",
       "      <td>5.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9980</td>\n",
       "      <td>6.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9990</td>\n",
       "      <td>5.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.964500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2019] 2023-03-16 19:35:48,381 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2821] 2023-03-16 19:35:48,391 >> Saving model checkpoint to /root/transformers/examples/pytorch/tmpdpkpzar3\n",
      "[INFO|configuration_utils.py:457] 2023-03-16 19:35:48,402 >> Configuration saved in /root/transformers/examples/pytorch/tmpdpkpzar3/config.json\n",
      "[INFO|modeling_utils.py:1762] 2023-03-16 19:36:00,212 >> Model weights saved in /root/transformers/examples/pytorch/tmpdpkpzar3/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2163] 2023-03-16 19:36:00,222 >> tokenizer config file saved in /root/transformers/examples/pytorch/tmpdpkpzar3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2170] 2023-03-16 19:36:00,230 >> Special tokens file saved in /root/transformers/examples/pytorch/tmpdpkpzar3/special_tokens_map.json\n",
      "[INFO|modelcard.py:449] 2023-03-16 19:36:00,867 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'adversarial_qa adversarialQA', 'type': 'adversarial_qa', 'config': 'adversarialQA', 'split': 'train', 'args': 'adversarialQA'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.3\n",
      "  total_flos               =  7299999GF\n",
      "  train_loss               =     5.9841\n",
      "  train_runtime            = 0:27:22.54\n",
      "  train_samples            =      30833\n",
      "  train_samples_per_second =     24.352\n",
      "  train_steps_per_second   =      6.088\n"
     ]
    }
   ],
   "source": [
    "tmp_dir = get_auto_remove_tmp_dir()\n",
    "testargs = f\"\"\"\n",
    "    run_qa.py\n",
    "    --model_name_or_path xlm-roberta-base\n",
    "    --version_2_with_negative\n",
    "    --dataset_name adversarial_qa\n",
    "    --dataset_config adversarialQA\n",
    "    --output_dir {tmp_dir}\n",
    "    --overwrite_output_dir\n",
    "    --max_steps=10000\n",
    "    --warmup_steps=0\n",
    "    --do_train\n",
    "    --learning_rate=5e-3\n",
    "    --per_device_train_batch_size=4\n",
    "    --per_device_eval_batch_size=2\n",
    "    --lr_scheduler_type greedy\n",
    "    --save_strategy no\n",
    "    --logging_steps 10\n",
    "    --min_lr=1e-5\n",
    "    --smooth True\n",
    "    --patience 10\n",
    "    --factor 0.9\n",
    "    --optim adafactor\n",
    "\"\"\".split()\n",
    "\n",
    "with patch.object(sys, \"argv\", testargs):\n",
    "    run_squad.main()\n",
    "    result = get_results(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919c2f59-2765-4f74-9860-d616bab549c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alllogs = json.load(open(f'{tmp_dir}/trainer_state.json'))\n",
    "d2 = [ (l['step'], l['learning_rate'], l['loss']) for l in alllogs['log_history'][:-1] ]\n",
    "\n",
    "!rm -r {tmp_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f641246a-0668-4fcd-a298-1f9eb9f68ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'LRs')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x1 = np.array(d1)[:,0]\n",
    "y1 = np.array(d1)[:,1]\n",
    "\n",
    "x2 = np.array(d2)[:,0] \n",
    "y2 = np.array(d2)[:,1]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "line1, = ax.plot(x1, y1, 'b-', label='baseline')\n",
    "line2, = ax.plot(x2, y2, 'r-', label='GreedyLR')\n",
    "plt.legend()\n",
    "plt.title('LRs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18e6e45a-246d-4aa5-83e0-f508cf5536ef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = np.array(d1)[:,0]\n",
    "y1 = np.array(d1)[:,2]\n",
    "\n",
    "x2 = np.array(d2)[:,0][::20]\n",
    "y2 = np.array(d2)[:,2][::20]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "line1, = ax.plot(x1, y1, 'b-', label='baseline')\n",
    "line2, = ax.plot(x2, y2, 'r-', label='GreedyLR')\n",
    "plt.legend()\n",
    "plt.title('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55deca4e-7319-4529-a03d-c27624fe977f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.013399999999999856"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1[-1][-1] - d2[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1e93b-a597-48e9-a65b-a785c2da0365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g5.16xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-gpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
